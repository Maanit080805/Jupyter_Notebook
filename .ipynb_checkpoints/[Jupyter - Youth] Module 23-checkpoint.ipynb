{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of text data\n",
    "\n",
    "One popular application of Natural Language processing is to classify text data. Classification can be done in various form: we can classify if a tweet is related to a particular topic or not, or we can classify if a particular review leans towards positive or negative. \n",
    "\n",
    "Today, we will hone our skills in classifying NLP data using NLP! \n",
    "Firstly, we will collect, analyze and process our data. We will be using bag of words and tf-idf (remember the activity that we have done in the acquire stage?). These processes turn texts into numbers. \n",
    "We will then take the word vectors we have created and use a machine learning algorithm to learn from the data and come up with a model to do our classification task. \n",
    "\n",
    "Let's start!\n",
    "\n",
    "For our first task, we will use a collection of tweet data to predict if the tweets are referring to natural disasters, or just regular tweets.\n",
    "\n",
    "Let's first import the required libraries!\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open csv file\n",
    "Do you have the tweet data file with you? If not, refer to Experience module 1 to see how you can download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tweet/disasters_social_media.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fe1949502822>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tweet/disasters_social_media.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1991\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1992\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tweet/disasters_social_media.csv'"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('./tweet/disasters_social_media.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our data for classification\n",
    "Before we begin working with the data, let us first look at some of the data's features. You should have an idea of the data structure from the data analysis in Experience 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778243823     True      golden                 156               NaN   \n",
       "1  778243824     True      golden                 152               NaN   \n",
       "2  778243825     True      golden                 137               NaN   \n",
       "3  778243826     True      golden                 136               NaN   \n",
       "4  778243827     True      golden                 138               NaN   \n",
       "\n",
       "  choose_one  choose_one:confidence choose_one_gold keyword location  \\\n",
       "0   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "1   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "2   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "3   Relevant                 0.9603        Relevant     NaN      NaN   \n",
       "4   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "\n",
       "                                                text  tweetid  userid  \n",
       "0                 Just happened a terrible car crash      1.0     NaN  \n",
       "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
       "2  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
       "3  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
       "4             Forest fire near La Ronge Sask. Canada     16.0     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head() function allows us to see the first few rows from the dataset. Do you wonder how many rows do we have here?\n",
    "\n",
    "### Task: print out the length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10876"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What headings do you see above? Which headings do you think are important?\n",
    "\n",
    "### What is a 'target'?\n",
    "We call the field we are trying to predict the 'target'. In this case, the target is whether the tweet is relevant to a natural disaster or irrelevant. These values are reflected in the `['choose_one']` column (See it above NOW!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the 'labels'?\n",
    "In this dataset, the labels of the target has been filled out by human volunteers. When you are working on your own datasets in the future, you may have to label them manually, or find volunteers to do so.\n",
    "\n",
    "This is usually an expensive task to do in terms of effort and time. There are even [online platforms](https://www.mturk.com/) where you can find workers for this job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check labels\n",
    "\n",
    "Let's look at the the categories that the tweets have been classified into. To do that, we can look for the number of unique values within that column. The python's built-in function `set()` takes in a list of values and outputs the total unique values. Let us see how it works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange', 'pears'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'pears'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the code above? You have only 3 unique values in a list of 5, and only the unit values are printed out. \n",
    "\n",
    "### Task: Change the function such that you have 2 unique values and 6 values in the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'orange','apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below list down the values of the column 'choose_one', which is a measure of relevance of a particular tweet to natural disaster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Relevant', 'Relevant', 'Relevant', ..., 'Relevant', 'Relevant',\n",
       "       'Relevant'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.choose_one.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using the set() funciton, can you guess how many potential values might this column have? \n",
    "\n",
    "### Task: find out the number of unique relevance values on the 'choose_one' column using the set() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Can't Decide\", 'Not Relevant', 'Relevant'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense? A tweet can either be related to a natural disaster, not related to a natural disaster, and there are cases when the people who labelled the data cannot decide if the tweet is related to natural disaster or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we only care about predicting in a binary fashion (relevant vs not relevant), so we discard the 'Can't decide' class. Remember how we [subset data using criteria](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html) on pandas dataframe?  \n",
    "\n",
    "### Task: Subset the dataframe and only take rows that does not have 'Can't Decide' in the choose_one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out your dataframe and see what you have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>778243828</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>778243831</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>778243832</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>778243833</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>778243834</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>Relevant\\nCan't Decide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>778243835</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>778243836</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>778243837</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>778243838</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9607</td>\n",
       "      <td>Relevant\\nCan't Decide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>778243839</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>778243840</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>Relevant\\nCan't Decide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>778243841</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>778243842</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>778243843</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>778243844</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.6761</td>\n",
       "      <td>Relevant\\nCan't Decide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>778243845</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>778243846</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>778243847</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>778243848</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>778243849</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>778243851</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>778243852</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>778243853</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>778243855</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>3.300000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>778243856</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10846</th>\n",
       "      <td>778261080</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9210</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heat wave warning aa? Ayyo dei. Just when I pl...</td>\n",
       "      <td>6.470000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10847</th>\n",
       "      <td>778261081</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An IS group suicide bomber detonated an explos...</td>\n",
       "      <td>9.670000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>778261082</td>\n",
       "      <td>False</td>\n",
       "      <td>golden</td>\n",
       "      <td>14</td>\n",
       "      <td>8/27/15 16:20</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just heard a really loud bang and everyone i...</td>\n",
       "      <td>7.807000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10849</th>\n",
       "      <td>778261083</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A gas thing just exploded and I heard screams ...</td>\n",
       "      <td>5.474575e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>778261084</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWS: Flash Flood Warning Continued for Shelby ...</td>\n",
       "      <td>3.546346e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10851</th>\n",
       "      <td>778261085</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
       "      <td>5.436346e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10852</th>\n",
       "      <td>778261086</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris foun...</td>\n",
       "      <td>5.436000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10853</th>\n",
       "      <td>778261087</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9204</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
       "      <td>2.453000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10854</th>\n",
       "      <td>778261088</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California ...</td>\n",
       "      <td>6.750000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>778261089</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8809</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>6.548750e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10856</th>\n",
       "      <td>778261090</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>See the 16yr old PKK suicide bomber who detona...</td>\n",
       "      <td>4.325675e+08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>778261091</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8022</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To conference attendees! The blue line from th...</td>\n",
       "      <td>2.345750e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>778261092</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The death toll in a #IS-suicide car bombing on...</td>\n",
       "      <td>4.326570e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>778261093</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>3.465000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10860</th>\n",
       "      <td>778261094</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>6.745362e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>778261095</td>\n",
       "      <td>False</td>\n",
       "      <td>golden</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/15 13:52</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>2.426350e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10862</th>\n",
       "      <td>778261096</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>6.545000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>778261097</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>8.580000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10864</th>\n",
       "      <td>778261098</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>5.245200e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10865</th>\n",
       "      <td>778261099</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>5.840000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10866</th>\n",
       "      <td>778261100</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>3.250000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>778261101</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>5.870000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>778261102</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9201</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>7.884500e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>778261103</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>4.234000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>778261104</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>5.640000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>778261105</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7629</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>5.675678e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>778261106</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>4.234000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>778261107</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>3.242000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>778261108</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>4.570000e+02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>778261109</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>6.585000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10860 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0      778243823     True      golden                 156               NaN   \n",
       "1      778243824     True      golden                 152               NaN   \n",
       "2      778243825     True      golden                 137               NaN   \n",
       "3      778243826     True      golden                 136               NaN   \n",
       "4      778243827     True      golden                 138               NaN   \n",
       "5      778243828     True      golden                 140               NaN   \n",
       "6      778243831     True      golden                 142               NaN   \n",
       "7      778243832     True      golden                 151               NaN   \n",
       "8      778243833     True      golden                 143               NaN   \n",
       "9      778243834     True      golden                 136               NaN   \n",
       "10     778243835     True      golden                 152               NaN   \n",
       "11     778243836     True      golden                 157               NaN   \n",
       "12     778243837     True      golden                 143               NaN   \n",
       "13     778243838     True      golden                 140               NaN   \n",
       "14     778243839     True      golden                 136               NaN   \n",
       "15     778243840     True      golden                 147               NaN   \n",
       "16     778243841     True      golden                 147               NaN   \n",
       "17     778243842     True      golden                 132               NaN   \n",
       "18     778243843     True      golden                 144               NaN   \n",
       "19     778243844     True      golden                 147               NaN   \n",
       "20     778243845     True      golden                 136               NaN   \n",
       "21     778243846     True      golden                 151               NaN   \n",
       "22     778243847     True      golden                 124               NaN   \n",
       "23     778243848     True      golden                 140               NaN   \n",
       "24     778243849     True      golden                 141               NaN   \n",
       "25     778243851     True      golden                 142               NaN   \n",
       "26     778243852     True      golden                 138               NaN   \n",
       "27     778243853     True      golden                 137               NaN   \n",
       "28     778243855     True      golden                 143               NaN   \n",
       "29     778243856     True      golden                 138               NaN   \n",
       "...          ...      ...         ...                 ...               ...   \n",
       "10846  778261080     True      golden                  96               NaN   \n",
       "10847  778261081     True      golden                  95               NaN   \n",
       "10848  778261082    False      golden                  14     8/27/15 16:20   \n",
       "10849  778261083     True      golden                  94               NaN   \n",
       "10850  778261084     True      golden                 104               NaN   \n",
       "10851  778261085     True      golden                  99               NaN   \n",
       "10852  778261086     True      golden                 102               NaN   \n",
       "10853  778261087     True      golden                  93               NaN   \n",
       "10854  778261088     True      golden                  95               NaN   \n",
       "10855  778261089     True      golden                  91               NaN   \n",
       "10856  778261090     True      golden                 101               NaN   \n",
       "10857  778261091     True      golden                  93               NaN   \n",
       "10858  778261092     True      golden                  88               NaN   \n",
       "10859  778261093     True      golden                 106               NaN   \n",
       "10860  778261094     True      golden                  94               NaN   \n",
       "10861  778261095    False      golden                   3     8/27/15 13:52   \n",
       "10862  778261096     True      golden                  92               NaN   \n",
       "10863  778261097     True      golden                  96               NaN   \n",
       "10864  778261098     True      golden                 102               NaN   \n",
       "10865  778261099     True      golden                 102               NaN   \n",
       "10866  778261100     True      golden                  96               NaN   \n",
       "10867  778261101     True      golden                  99               NaN   \n",
       "10868  778261102     True      golden                 113               NaN   \n",
       "10869  778261103     True      golden                  99               NaN   \n",
       "10870  778261104     True      golden                 104               NaN   \n",
       "10871  778261105     True      golden                 100               NaN   \n",
       "10872  778261106     True      golden                  90               NaN   \n",
       "10873  778261107     True      golden                 102               NaN   \n",
       "10874  778261108     True      golden                  96               NaN   \n",
       "10875  778261109     True      golden                  97               NaN   \n",
       "\n",
       "         choose_one  choose_one:confidence         choose_one_gold keyword  \\\n",
       "0          Relevant                 1.0000                Relevant     NaN   \n",
       "1          Relevant                 1.0000                Relevant     NaN   \n",
       "2          Relevant                 1.0000                Relevant     NaN   \n",
       "3          Relevant                 0.9603                Relevant     NaN   \n",
       "4          Relevant                 1.0000                Relevant     NaN   \n",
       "5          Relevant                 1.0000                Relevant     NaN   \n",
       "6          Relevant                 1.0000                Relevant     NaN   \n",
       "7          Relevant                 1.0000                Relevant     NaN   \n",
       "8          Relevant                 1.0000                Relevant     NaN   \n",
       "9          Relevant                 0.9606  Relevant\\nCan't Decide     NaN   \n",
       "10         Relevant                 1.0000                Relevant     NaN   \n",
       "11         Relevant                 1.0000                Relevant     NaN   \n",
       "12         Relevant                 1.0000                Relevant     NaN   \n",
       "13         Relevant                 0.9607  Relevant\\nCan't Decide     NaN   \n",
       "14         Relevant                 0.9215                Relevant     NaN   \n",
       "15         Relevant                 0.7177  Relevant\\nCan't Decide     NaN   \n",
       "16         Relevant                 0.9603                Relevant     NaN   \n",
       "17         Relevant                 1.0000                Relevant     NaN   \n",
       "18         Relevant                 1.0000                Relevant     NaN   \n",
       "19         Relevant                 0.6761  Relevant\\nCan't Decide     NaN   \n",
       "20         Relevant                 1.0000                Relevant     NaN   \n",
       "21     Not Relevant                 0.9612            Not Relevant     NaN   \n",
       "22     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "23     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "24     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "25     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "26     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "27     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "28     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "29     Not Relevant                 1.0000            Not Relevant     NaN   \n",
       "...             ...                    ...                     ...     ...   \n",
       "10846      Relevant                 0.9210                Relevant     NaN   \n",
       "10847      Relevant                 1.0000                Relevant     NaN   \n",
       "10848  Not Relevant                 0.5696                Relevant     NaN   \n",
       "10849      Relevant                 1.0000                Relevant     NaN   \n",
       "10850      Relevant                 1.0000                Relevant     NaN   \n",
       "10851      Relevant                 0.9595                Relevant     NaN   \n",
       "10852      Relevant                 1.0000                Relevant     NaN   \n",
       "10853      Relevant                 0.9204                Relevant     NaN   \n",
       "10854      Relevant                 0.9608                Relevant     NaN   \n",
       "10855      Relevant                 0.8809                Relevant     NaN   \n",
       "10856      Relevant                 1.0000                Relevant     NaN   \n",
       "10857      Relevant                 0.8022                Relevant     NaN   \n",
       "10858      Relevant                 1.0000                Relevant     NaN   \n",
       "10859      Relevant                 1.0000                Relevant     NaN   \n",
       "10860      Relevant                 0.8018                Relevant     NaN   \n",
       "10861  Not Relevant                 0.6678                Relevant     NaN   \n",
       "10862      Relevant                 0.9601                Relevant     NaN   \n",
       "10863      Relevant                 0.9601                Relevant     NaN   \n",
       "10864      Relevant                 0.8807                Relevant     NaN   \n",
       "10865      Relevant                 1.0000                Relevant     NaN   \n",
       "10866      Relevant                 1.0000                Relevant     NaN   \n",
       "10867      Relevant                 0.9601                Relevant     NaN   \n",
       "10868      Relevant                 0.9201                Relevant     NaN   \n",
       "10869      Relevant                 1.0000                Relevant     NaN   \n",
       "10870      Relevant                 1.0000                Relevant     NaN   \n",
       "10871      Relevant                 0.7629                Relevant     NaN   \n",
       "10872      Relevant                 0.9203                Relevant     NaN   \n",
       "10873      Relevant                 1.0000                Relevant     NaN   \n",
       "10874      Relevant                 0.8419                Relevant     NaN   \n",
       "10875      Relevant                 0.8812                Relevant     NaN   \n",
       "\n",
       "      location                                               text  \\\n",
       "0          NaN                 Just happened a terrible car crash   \n",
       "1          NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "2          NaN  Heard about #earthquake is different cities, s...   \n",
       "3          NaN  there is a forest fire at spot pond, geese are...   \n",
       "4          NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5          NaN  All residents asked to 'shelter in place' are ...   \n",
       "6          NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7          NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "8          NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "9          NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "10         NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "11         NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "12         NaN                 We're shaking...It's an earthquake   \n",
       "13         NaN  I'm on top of the hill and I can see a fire in...   \n",
       "14         NaN  There's an emergency evacuation happening now ...   \n",
       "15         NaN  I'm afraid that the tornado is coming to our a...   \n",
       "16         NaN        Three people died from the heat wave so far   \n",
       "17         NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "18         NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "19         NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "20         NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "21         NaN  They'd probably still show more life than Arse...   \n",
       "22         NaN                                  Hey! How are you?   \n",
       "23         NaN                                     What's up man?   \n",
       "24         NaN                                      I love fruits   \n",
       "25         NaN                                   Summer is lovely   \n",
       "26         NaN                                  My car is so fast   \n",
       "27         NaN                                   What a nice hat?   \n",
       "28         NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "29         NaN                                          Fuck off!   \n",
       "...        ...                                                ...   \n",
       "10846      NaN  Heat wave warning aa? Ayyo dei. Just when I pl...   \n",
       "10847      NaN  An IS group suicide bomber detonated an explos...   \n",
       "10848      NaN  I just heard a really loud bang and everyone i...   \n",
       "10849      NaN  A gas thing just exploded and I heard screams ...   \n",
       "10850      NaN  NWS: Flash Flood Warning Continued for Shelby ...   \n",
       "10851      NaN  RT @LivingSafely: #NWS issues Severe #Thunders...   \n",
       "10852      NaN  #??? #?? #??? #??? MH370: Aircraft debris foun...   \n",
       "10853      NaN  Father-of-three Lost Control of Car After Over...   \n",
       "10854      NaN  1.3 #Earthquake in 9Km Ssw Of Anza California ...   \n",
       "10855      NaN  Evacuation order lifted for town of Roosevelt:...   \n",
       "10856      NaN  See the 16yr old PKK suicide bomber who detona...   \n",
       "10857      NaN  To conference attendees! The blue line from th...   \n",
       "10858      NaN  The death toll in a #IS-suicide car bombing on...   \n",
       "10859      NaN  #breaking #LA Refugio oil spill may have been ...   \n",
       "10860      NaN  a siren just went off and it wasn't the Forney...   \n",
       "10861      NaN  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "10862      NaN  Officials say a quarantine is in place at an A...   \n",
       "10863      NaN  #WorldNews Fallen powerlines on G:link tram: U...   \n",
       "10864      NaN  on the flip side I'm at Walmart and there is a...   \n",
       "10865      NaN  Storm in RI worse than last hurricane. My city...   \n",
       "10866      NaN  Suicide bomber kills 15 in Saudi security site...   \n",
       "10867      NaN  #stormchase Violent Record Breaking EF-5 El Re...   \n",
       "10868      NaN  Green Line derailment in Chicago http://t.co/U...   \n",
       "10869      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "10870      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "10871      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872      NaN  Police investigating after an e-bike collided ...   \n",
       "10873      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "10874      NaN  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "10875      NaN  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "            tweetid  userid  \n",
       "0      1.000000e+00     NaN  \n",
       "1      1.300000e+01     NaN  \n",
       "2      1.400000e+01     NaN  \n",
       "3      1.500000e+01     NaN  \n",
       "4      1.600000e+01     NaN  \n",
       "5      1.700000e+01     NaN  \n",
       "6      1.800000e+01     NaN  \n",
       "7      1.900000e+01     NaN  \n",
       "8      2.000000e+01     NaN  \n",
       "9      2.100000e+01     NaN  \n",
       "10     1.100000e+01     NaN  \n",
       "11     1.200000e+01     NaN  \n",
       "12     2.000000e+00     NaN  \n",
       "13     7.000000e+00     NaN  \n",
       "14     8.000000e+00     NaN  \n",
       "15     9.000000e+00     NaN  \n",
       "16     1.000000e+01     NaN  \n",
       "17     2.200000e+01     NaN  \n",
       "18     2.300000e+01     NaN  \n",
       "19     2.400000e+01     NaN  \n",
       "20     2.500000e+01     NaN  \n",
       "21     2.600000e+01     NaN  \n",
       "22     2.700000e+01     NaN  \n",
       "23     2.800000e+01     NaN  \n",
       "24     2.900000e+01     NaN  \n",
       "25     3.000000e+01     NaN  \n",
       "26     3.100000e+01     NaN  \n",
       "27     3.200000e+01     NaN  \n",
       "28     3.300000e+01     NaN  \n",
       "29     3.400000e+01     NaN  \n",
       "...             ...     ...  \n",
       "10846  6.470000e+02     NaN  \n",
       "10847  9.670000e+02     NaN  \n",
       "10848  7.807000e+03     NaN  \n",
       "10849  5.474575e+10     NaN  \n",
       "10850  3.546346e+10     NaN  \n",
       "10851  5.436346e+07     NaN  \n",
       "10852  5.436000e+03     NaN  \n",
       "10853  2.453000e+03     NaN  \n",
       "10854  6.750000e+02     NaN  \n",
       "10855  6.548750e+05     NaN  \n",
       "10856  4.325675e+08     NaN  \n",
       "10857  2.345750e+05     NaN  \n",
       "10858  4.326570e+05     NaN  \n",
       "10859  3.465000e+03     NaN  \n",
       "10860  6.745362e+07     NaN  \n",
       "10861  2.426350e+05     NaN  \n",
       "10862  6.545000e+03     NaN  \n",
       "10863  8.580000e+02     NaN  \n",
       "10864  5.245200e+04     NaN  \n",
       "10865  5.840000e+02     NaN  \n",
       "10866  3.250000e+02     NaN  \n",
       "10867  5.870000e+02     NaN  \n",
       "10868  7.884500e+04     NaN  \n",
       "10869  4.234000e+03     NaN  \n",
       "10870  5.640000e+02     NaN  \n",
       "10871  5.675678e+06     NaN  \n",
       "10872  4.234000e+03     NaN  \n",
       "10873  3.242000e+03     NaN  \n",
       "10874  4.570000e+02     NaN  \n",
       "10875  6.585000e+03     NaN  \n",
       "\n",
       "[10860 rows x 13 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the length of your dataframe now. Does it decreases or stay the same?\n",
    "\n",
    "### Print out the length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10860"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to focus on only columns 'text' and 'choose_one'\n",
    "\n",
    "### Task: Subset the dataframe to only take the columns 'text' and 'choose_one'\n",
    "See Selecting Data Using Labels (Column Headings) from this [article](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What's up man?</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I love fruits</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10846</th>\n",
       "      <td>Heat wave warning aa? Ayyo dei. Just when I pl...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10847</th>\n",
       "      <td>An IS group suicide bomber detonated an explos...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>I just heard a really loud bang and everyone i...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10849</th>\n",
       "      <td>A gas thing just exploded and I heard screams ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>NWS: Flash Flood Warning Continued for Shelby ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10851</th>\n",
       "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10852</th>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris foun...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10853</th>\n",
       "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10854</th>\n",
       "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10856</th>\n",
       "      <td>See the 16yr old PKK suicide bomber who detona...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>To conference attendees! The blue line from th...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>The death toll in a #IS-suicide car bombing on...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10860</th>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10862</th>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10864</th>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10865</th>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10866</th>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    choose_one\n",
       "0                     Just happened a terrible car crash      Relevant\n",
       "1      Our Deeds are the Reason of this #earthquake M...      Relevant\n",
       "2      Heard about #earthquake is different cities, s...      Relevant\n",
       "3      there is a forest fire at spot pond, geese are...      Relevant\n",
       "4                 Forest fire near La Ronge Sask. Canada      Relevant\n",
       "5      All residents asked to 'shelter in place' are ...      Relevant\n",
       "6      13,000 people receive #wildfires evacuation or...      Relevant\n",
       "7      Just got sent this photo from Ruby #Alaska as ...      Relevant\n",
       "8      #RockyFire Update => California Hwy. 20 closed...      Relevant\n",
       "9               Apocalypse lighting. #Spokane #wildfires      Relevant\n",
       "10     #flood #disaster Heavy rain causes flash flood...      Relevant\n",
       "11         Typhoon Soudelor kills 28 in China and Taiwan      Relevant\n",
       "12                    We're shaking...It's an earthquake      Relevant\n",
       "13     I'm on top of the hill and I can see a fire in...      Relevant\n",
       "14     There's an emergency evacuation happening now ...      Relevant\n",
       "15     I'm afraid that the tornado is coming to our a...      Relevant\n",
       "16           Three people died from the heat wave so far      Relevant\n",
       "17     Haha South Tampa is getting flooded hah- WAIT ...      Relevant\n",
       "18     #raining #flooding #Florida #TampaBay #Tampa 1...      Relevant\n",
       "19               #Flood in Bago Myanmar #We arrived Bago      Relevant\n",
       "20     Damage to school bus on 80 in multi car crash ...      Relevant\n",
       "21     They'd probably still show more life than Arse...  Not Relevant\n",
       "22                                     Hey! How are you?  Not Relevant\n",
       "23                                        What's up man?  Not Relevant\n",
       "24                                         I love fruits  Not Relevant\n",
       "25                                      Summer is lovely  Not Relevant\n",
       "26                                     My car is so fast  Not Relevant\n",
       "27                                      What a nice hat?  Not Relevant\n",
       "28                          What a goooooooaaaaaal!!!!!!  Not Relevant\n",
       "29                                             Fuck off!  Not Relevant\n",
       "...                                                  ...           ...\n",
       "10846  Heat wave warning aa? Ayyo dei. Just when I pl...      Relevant\n",
       "10847  An IS group suicide bomber detonated an explos...      Relevant\n",
       "10848  I just heard a really loud bang and everyone i...  Not Relevant\n",
       "10849  A gas thing just exploded and I heard screams ...      Relevant\n",
       "10850  NWS: Flash Flood Warning Continued for Shelby ...      Relevant\n",
       "10851  RT @LivingSafely: #NWS issues Severe #Thunders...      Relevant\n",
       "10852  #??? #?? #??? #??? MH370: Aircraft debris foun...      Relevant\n",
       "10853  Father-of-three Lost Control of Car After Over...      Relevant\n",
       "10854  1.3 #Earthquake in 9Km Ssw Of Anza California ...      Relevant\n",
       "10855  Evacuation order lifted for town of Roosevelt:...      Relevant\n",
       "10856  See the 16yr old PKK suicide bomber who detona...      Relevant\n",
       "10857  To conference attendees! The blue line from th...      Relevant\n",
       "10858  The death toll in a #IS-suicide car bombing on...      Relevant\n",
       "10859  #breaking #LA Refugio oil spill may have been ...      Relevant\n",
       "10860  a siren just went off and it wasn't the Forney...      Relevant\n",
       "10861  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  Not Relevant\n",
       "10862  Officials say a quarantine is in place at an A...      Relevant\n",
       "10863  #WorldNews Fallen powerlines on G:link tram: U...      Relevant\n",
       "10864  on the flip side I'm at Walmart and there is a...      Relevant\n",
       "10865  Storm in RI worse than last hurricane. My city...      Relevant\n",
       "10866  Suicide bomber kills 15 in Saudi security site...      Relevant\n",
       "10867  #stormchase Violent Record Breaking EF-5 El Re...      Relevant\n",
       "10868  Green Line derailment in Chicago http://t.co/U...      Relevant\n",
       "10869  Two giant cranes holding a bridge collapse int...      Relevant\n",
       "10870  @aria_ahrary @TheTawniest The out of control w...      Relevant\n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...      Relevant\n",
       "10872  Police investigating after an e-bike collided ...      Relevant\n",
       "10873  The Latest: More Homes Razed by Northern Calif...      Relevant\n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...      Relevant\n",
       "10875  #CityofCalgary has activated its Municipal Eme...      Relevant\n",
       "\n",
       "[10860 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also [map](https://chrisalbon.com/python/data_wrangling/pandas_map_values_to_values/) these values on to numbers 1 for relevant tweets, and 0 for irrelevant tweets. \n",
    "\n",
    "### Task: Map 'Relevant' into 1 and 'Irrelevant' into 0 and put it into a new column called 'relevance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "relevance = {#your code here}\n",
    "df['relevance'] = df.choose_one.map(relevance) # map relevant values to the number 1, and not relevant to the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What's up man?</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I love fruits</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10846</th>\n",
       "      <td>Heat wave warning aa? Ayyo dei. Just when I pl...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10847</th>\n",
       "      <td>An IS group suicide bomber detonated an explos...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>I just heard a really loud bang and everyone i...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10849</th>\n",
       "      <td>A gas thing just exploded and I heard screams ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>NWS: Flash Flood Warning Continued for Shelby ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10851</th>\n",
       "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10852</th>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris foun...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10853</th>\n",
       "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10854</th>\n",
       "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10856</th>\n",
       "      <td>See the 16yr old PKK suicide bomber who detona...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>To conference attendees! The blue line from th...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>The death toll in a #IS-suicide car bombing on...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10860</th>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10862</th>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10864</th>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10865</th>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10866</th>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    choose_one  \\\n",
       "0                     Just happened a terrible car crash      Relevant   \n",
       "1      Our Deeds are the Reason of this #earthquake M...      Relevant   \n",
       "2      Heard about #earthquake is different cities, s...      Relevant   \n",
       "3      there is a forest fire at spot pond, geese are...      Relevant   \n",
       "4                 Forest fire near La Ronge Sask. Canada      Relevant   \n",
       "5      All residents asked to 'shelter in place' are ...      Relevant   \n",
       "6      13,000 people receive #wildfires evacuation or...      Relevant   \n",
       "7      Just got sent this photo from Ruby #Alaska as ...      Relevant   \n",
       "8      #RockyFire Update => California Hwy. 20 closed...      Relevant   \n",
       "9               Apocalypse lighting. #Spokane #wildfires      Relevant   \n",
       "10     #flood #disaster Heavy rain causes flash flood...      Relevant   \n",
       "11         Typhoon Soudelor kills 28 in China and Taiwan      Relevant   \n",
       "12                    We're shaking...It's an earthquake      Relevant   \n",
       "13     I'm on top of the hill and I can see a fire in...      Relevant   \n",
       "14     There's an emergency evacuation happening now ...      Relevant   \n",
       "15     I'm afraid that the tornado is coming to our a...      Relevant   \n",
       "16           Three people died from the heat wave so far      Relevant   \n",
       "17     Haha South Tampa is getting flooded hah- WAIT ...      Relevant   \n",
       "18     #raining #flooding #Florida #TampaBay #Tampa 1...      Relevant   \n",
       "19               #Flood in Bago Myanmar #We arrived Bago      Relevant   \n",
       "20     Damage to school bus on 80 in multi car crash ...      Relevant   \n",
       "21     They'd probably still show more life than Arse...  Not Relevant   \n",
       "22                                     Hey! How are you?  Not Relevant   \n",
       "23                                        What's up man?  Not Relevant   \n",
       "24                                         I love fruits  Not Relevant   \n",
       "25                                      Summer is lovely  Not Relevant   \n",
       "26                                     My car is so fast  Not Relevant   \n",
       "27                                      What a nice hat?  Not Relevant   \n",
       "28                          What a goooooooaaaaaal!!!!!!  Not Relevant   \n",
       "29                                             Fuck off!  Not Relevant   \n",
       "...                                                  ...           ...   \n",
       "10846  Heat wave warning aa? Ayyo dei. Just when I pl...      Relevant   \n",
       "10847  An IS group suicide bomber detonated an explos...      Relevant   \n",
       "10848  I just heard a really loud bang and everyone i...  Not Relevant   \n",
       "10849  A gas thing just exploded and I heard screams ...      Relevant   \n",
       "10850  NWS: Flash Flood Warning Continued for Shelby ...      Relevant   \n",
       "10851  RT @LivingSafely: #NWS issues Severe #Thunders...      Relevant   \n",
       "10852  #??? #?? #??? #??? MH370: Aircraft debris foun...      Relevant   \n",
       "10853  Father-of-three Lost Control of Car After Over...      Relevant   \n",
       "10854  1.3 #Earthquake in 9Km Ssw Of Anza California ...      Relevant   \n",
       "10855  Evacuation order lifted for town of Roosevelt:...      Relevant   \n",
       "10856  See the 16yr old PKK suicide bomber who detona...      Relevant   \n",
       "10857  To conference attendees! The blue line from th...      Relevant   \n",
       "10858  The death toll in a #IS-suicide car bombing on...      Relevant   \n",
       "10859  #breaking #LA Refugio oil spill may have been ...      Relevant   \n",
       "10860  a siren just went off and it wasn't the Forney...      Relevant   \n",
       "10861  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  Not Relevant   \n",
       "10862  Officials say a quarantine is in place at an A...      Relevant   \n",
       "10863  #WorldNews Fallen powerlines on G:link tram: U...      Relevant   \n",
       "10864  on the flip side I'm at Walmart and there is a...      Relevant   \n",
       "10865  Storm in RI worse than last hurricane. My city...      Relevant   \n",
       "10866  Suicide bomber kills 15 in Saudi security site...      Relevant   \n",
       "10867  #stormchase Violent Record Breaking EF-5 El Re...      Relevant   \n",
       "10868  Green Line derailment in Chicago http://t.co/U...      Relevant   \n",
       "10869  Two giant cranes holding a bridge collapse int...      Relevant   \n",
       "10870  @aria_ahrary @TheTawniest The out of control w...      Relevant   \n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...      Relevant   \n",
       "10872  Police investigating after an e-bike collided ...      Relevant   \n",
       "10873  The Latest: More Homes Razed by Northern Calif...      Relevant   \n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...      Relevant   \n",
       "10875  #CityofCalgary has activated its Municipal Eme...      Relevant   \n",
       "\n",
       "       relevance  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              1  \n",
       "10             1  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             1  \n",
       "15             1  \n",
       "16             1  \n",
       "17             1  \n",
       "18             1  \n",
       "19             1  \n",
       "20             1  \n",
       "21             0  \n",
       "22             0  \n",
       "23             0  \n",
       "24             0  \n",
       "25             0  \n",
       "26             0  \n",
       "27             0  \n",
       "28             0  \n",
       "29             0  \n",
       "...          ...  \n",
       "10846          1  \n",
       "10847          1  \n",
       "10848          0  \n",
       "10849          1  \n",
       "10850          1  \n",
       "10851          1  \n",
       "10852          1  \n",
       "10853          1  \n",
       "10854          1  \n",
       "10855          1  \n",
       "10856          1  \n",
       "10857          1  \n",
       "10858          1  \n",
       "10859          1  \n",
       "10860          1  \n",
       "10861          0  \n",
       "10862          1  \n",
       "10863          1  \n",
       "10864          1  \n",
       "10865          1  \n",
       "10866          1  \n",
       "10867          1  \n",
       "10868          1  \n",
       "10869          1  \n",
       "10870          1  \n",
       "10871          1  \n",
       "10872          1  \n",
       "10873          1  \n",
       "10874          1  \n",
       "10875          1  \n",
       "\n",
       "[10860 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what we have done!\n",
    "- We've chosen only the column we are interested in, reducing the columns from 13 to just 3!\n",
    "- We've mapped 'Relevant' to 1 and 'Not Relevant' to 0\n",
    "\n",
    "Now, we will proceed with text processing, followed with bag of words and tf-idf!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step is to write functions to normalize and tokenize the tweets (We covered this in Experience 1). An example is given below, but you can and should improve it by utilizing the skills you have learnt previously. For example, by adding additional ignore-words, or by lemming or stemming the dataset. The more you pre-process the data, the better your model can be!  \n",
    "  \n",
    "Why do you think we choose to ignore those words in the ignore list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned \n",
    "\n",
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(#your code here))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Add at least 5 more stop words into the extract_words function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test your new stop words on the same sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(#your code here))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do you notice that the word 'it' is still present? do you know why?\n",
    "\n",
    "### Task: add a function to lower the case in extract_words function\n",
    "See [here](https://machinelearningmastery.com/clean-text-machine-learning-python/) on how to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
    "    words = #your code here\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again with the same sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'today', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Feel free to add your own processing into the pipeline e.g. stemming or lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of words\n",
    "Now that we have a function to pre-process our textual data, we can proceed with converting our textual data into numbers. The simplest way to do this is using the bag of words algorithm. \n",
    "\n",
    "In a bag of words, we count the number of times each word appears for each tweet and use those counts as our input data. This is accomplished in the following steps:\n",
    "1. Create a vocabulary of all words that appear in your corpus (a corpus is a collection of all your text data, i.e. all tweets)\n",
    "2. Turn that vocabulary into a vector. i.e. if there are 500 unique words in your corpus, the vector will have a length of 500, with each position corresponding to a word in the corpus.\n",
    "3. For each document (tweet), count the number of times every word appears and add those numbers into the vector. This will result in each document having its own vector of length 500, that represents all the words appearing in the document.\n",
    "\n",
    "### Example \n",
    "consider a corpus consisting of two documents: \n",
    "1. 'I love NLP', \n",
    "2. 'I love machine learning'. \n",
    "\n",
    "#### Vocbulary\n",
    "The vocabulary will be a vector of length 5, consisting of the words: \n",
    "\n",
    "'I', 'love', 'NLP', 'machine', 'learning'.  \n",
    "\n",
    "#### Vector\n",
    "The vector for the first sentence (number 1) will be: \n",
    "[1, 1, 1, 0, 0] since it contains 'I', 'love', 'NLP', but not 'machine', and 'learning'.  \n",
    "\n",
    "Can you construct the vector for 2?  \n",
    "\n",
    "Combining vectors for 1 and 2, the bag of words will be an array with vector 1 in the first row, and vector 2 in the second row. \n",
    "\n",
    "Now let us implement this algorithm!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the bag of words\n",
    "\n",
    "First, we would like to know how often each individual word appears in our dataset. \n",
    "\n",
    "We can represent this in the form of a dictionary, which has the format {'word':frequency}, where each key is a word, and the frequencies are the number of times the words appears in our dataset. \n",
    "\n",
    "If you would like to learn more about dictionaries, visit [python dictionaries](https://www.w3schools.com/python/python_dictionaries.asp).  \n",
    "\n",
    "### Hash map\n",
    "\n",
    "This dictionary is known as a hash map, and it can be built progressively by looping through each token in the document. \n",
    "\n",
    "If the token can not be found in the hash map, add the token to the hash map, and set it's frequency as 1. If the token already exists, increment the frequency by 1.  \n",
    "  \n",
    "We will do this in two functions. \n",
    "1. First, build a function called map_book that takes in a dictionary called the hash_map, as well as the tokens from a tweet, and updates the hash_map with each word in the tokens. \n",
    "2. Next, build a function (you can call it make_hash_map) that can loop through all the tweets, and calls the first function to update the hash_map.  \n",
    "  \n",
    "*Hint: You can loop through your tokens by using `for word in tokens:`.  \n",
    "You can check if the word exists in your hash_map by using `if word in hash_map:`  \n",
    "you can assess the counts of your hash map by using `hash_map[word]`. Increment this by 1 to increase the count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function map_book. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequency of words\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = #your code here\n",
    "            else:\n",
    "                hash_map[word] = #your code here\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function make_hash_map. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_map(df):\n",
    "    hash_map = {}\n",
    "    for index, row in df.iterrows():\n",
    "        hash_map = map_book(hash_map, extract_words(row['text']))\n",
    "    return hash_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining our dictionary\n",
    "\n",
    "While you can construct your bag of words using all words in all tweets, it can become too much for your computer very quickly. A good solution is to take just a few hundred or thousand of the most common words. We will redefine our dictionary to consist of just the 500 most popular tokens.  \n",
    "  \n",
    "How can we do this? Remember that we have just constructed a hash map which is a dictionary with each token as a key, and the number of times the token has appeared as the value. Build a function called frequent_vocab that takes in the hash_map and the maximum vocabulary, and returns a list of the most popular tokens as defined by the maximum vocabulary (set this to 500 for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function frequent_vocab. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function frequent_vocab with the following input: word_freq and max_features\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  #initialize counter with the value zero\n",
    "    vocab = []   # create an empty list called vocab\n",
    "    # list words in the dictionary in descending order of frequency\n",
    "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
    "       #loop function to get the top (max_features) number of words\n",
    "        if counter<max_features: \n",
    "            vocab.append(key)\n",
    "            counter+=1\n",
    "        else: break\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment! What happens if you change the above function to (reverse = False)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'co',\n",
       " 'http',\n",
       " 'in',\n",
       " 'i',\n",
       " 's',\n",
       " 'for',\n",
       " 'on',\n",
       " 'that',\n",
       " 'with',\n",
       " 'by',\n",
       " 'at',\n",
       " 'this',\n",
       " 'https',\n",
       " 'from',\n",
       " 'be',\n",
       " 'was',\n",
       " 'û_',\n",
       " 'have',\n",
       " 'amp',\n",
       " 'like',\n",
       " 'as',\n",
       " 'up',\n",
       " 'just',\n",
       " 'me',\n",
       " 'we',\n",
       " 'but',\n",
       " 'so',\n",
       " 'not',\n",
       " 'your',\n",
       " 'm',\n",
       " 'out',\n",
       " 'no',\n",
       " 'all',\n",
       " 'will',\n",
       " 'after',\n",
       " 'fire',\n",
       " 'can',\n",
       " 'an',\n",
       " 'when',\n",
       " 'has',\n",
       " 'get',\n",
       " 'new',\n",
       " 'via',\n",
       " 'they',\n",
       " 'more',\n",
       " 'about',\n",
       " 'what',\n",
       " '2',\n",
       " 'now',\n",
       " 'or',\n",
       " 'news',\n",
       " 'people',\n",
       " 'one',\n",
       " 'who',\n",
       " 'there',\n",
       " 'over',\n",
       " 'don',\n",
       " 'been',\n",
       " 'do',\n",
       " 'into',\n",
       " 're',\n",
       " 'emergency',\n",
       " 'video',\n",
       " 'disaster',\n",
       " 'would',\n",
       " '3',\n",
       " 'police',\n",
       " 'her',\n",
       " 'his',\n",
       " 'u',\n",
       " 'than',\n",
       " 'were',\n",
       " 'still',\n",
       " 'some',\n",
       " 'body',\n",
       " 'suicide',\n",
       " 'us',\n",
       " '1',\n",
       " 'why',\n",
       " 'storm',\n",
       " 'off',\n",
       " 'first',\n",
       " 'time',\n",
       " 'burning',\n",
       " 'crash',\n",
       " 'them',\n",
       " 'rt',\n",
       " 'attack',\n",
       " 'had',\n",
       " 'got',\n",
       " 'back',\n",
       " 'california',\n",
       " 'day',\n",
       " 'know',\n",
       " 'fires',\n",
       " 'two',\n",
       " 'man',\n",
       " 'our',\n",
       " 'buildings',\n",
       " 'going',\n",
       " 'today',\n",
       " 'see',\n",
       " 'world',\n",
       " 'nuclear',\n",
       " 'bomb',\n",
       " 'love',\n",
       " 'hiroshima',\n",
       " 'year',\n",
       " 'full',\n",
       " 'go',\n",
       " '5',\n",
       " 'û',\n",
       " 'dead',\n",
       " 'youtube',\n",
       " 'life',\n",
       " 'watch',\n",
       " 'old',\n",
       " 'car',\n",
       " 'their',\n",
       " 'killed',\n",
       " 'train',\n",
       " 'think',\n",
       " '4',\n",
       " 'only',\n",
       " 'last',\n",
       " 'ûªs',\n",
       " 'here',\n",
       " 'down',\n",
       " 'accident',\n",
       " 'let',\n",
       " 'war',\n",
       " 'good',\n",
       " 'make',\n",
       " 'being',\n",
       " 'say',\n",
       " 'gt',\n",
       " '2015',\n",
       " 'w',\n",
       " 'could',\n",
       " 'way',\n",
       " 'may',\n",
       " 'many',\n",
       " 'families',\n",
       " 'need',\n",
       " 'even',\n",
       " 'years',\n",
       " 'want',\n",
       " 'best',\n",
       " 'because',\n",
       " 've',\n",
       " 'then',\n",
       " 'mass',\n",
       " 'll',\n",
       " 'did',\n",
       " 'bombing',\n",
       " 'too',\n",
       " 'collapse',\n",
       " 'right',\n",
       " 'home',\n",
       " 'him',\n",
       " 'water',\n",
       " 'forest',\n",
       " 'death',\n",
       " 'd',\n",
       " 'army',\n",
       " 'should',\n",
       " 'take',\n",
       " 'its',\n",
       " 'work',\n",
       " 'black',\n",
       " 'another',\n",
       " 'really',\n",
       " 'please',\n",
       " 'lol',\n",
       " 'wildfire',\n",
       " 'never',\n",
       " 'hot',\n",
       " 'god',\n",
       " 'read',\n",
       " 'mh370',\n",
       " 'look',\n",
       " 'help',\n",
       " 'bomber',\n",
       " 'fatal',\n",
       " 'am',\n",
       " 'any',\n",
       " 'live',\n",
       " 'where',\n",
       " 'northern',\n",
       " 'those',\n",
       " 'obama',\n",
       " '8',\n",
       " 'pm',\n",
       " 'japan',\n",
       " '11',\n",
       " 'school',\n",
       " 'much',\n",
       " 'feel',\n",
       " 'city',\n",
       " 'wild',\n",
       " 'flood',\n",
       " 'reddit',\n",
       " 'great',\n",
       " '9',\n",
       " 'stop',\n",
       " 'near',\n",
       " 'night',\n",
       " 'shit',\n",
       " 'said',\n",
       " 'latest',\n",
       " 'injured',\n",
       " 'before',\n",
       " '6',\n",
       " 'typhoon',\n",
       " 'top',\n",
       " 'homes',\n",
       " 'services',\n",
       " 'floods',\n",
       " 'under',\n",
       " 'hope',\n",
       " 'fear',\n",
       " 'ever',\n",
       " 'during',\n",
       " 'come',\n",
       " 'legionnaires',\n",
       " 'atomic',\n",
       " 'while',\n",
       " 'im',\n",
       " 'house',\n",
       " 'flames',\n",
       " '10',\n",
       " 'truck',\n",
       " 'state',\n",
       " 'post',\n",
       " 'getting',\n",
       " '15',\n",
       " 'wreck',\n",
       " 'ass',\n",
       " 'everyone',\n",
       " 'every',\n",
       " 'damage',\n",
       " 'content',\n",
       " 'change',\n",
       " 'red',\n",
       " 'earthquake',\n",
       " 'cross',\n",
       " 'summer',\n",
       " 'since',\n",
       " 'severe',\n",
       " 'plan',\n",
       " 'oil',\n",
       " 'coming',\n",
       " 'again',\n",
       " '7',\n",
       " 'weather',\n",
       " 'these',\n",
       " 'military',\n",
       " 'found',\n",
       " 'food',\n",
       " 'ûò',\n",
       " 'thunderstorm',\n",
       " 'heat',\n",
       " 'family',\n",
       " 'debris',\n",
       " 'without',\n",
       " 'other',\n",
       " 'next',\n",
       " 'little',\n",
       " 'wounded',\n",
       " 'through',\n",
       " 'natural',\n",
       " 'most',\n",
       " 'lightning',\n",
       " 'hit',\n",
       " 'gonna',\n",
       " 'evacuation',\n",
       " 'devastated',\n",
       " 'cause',\n",
       " 'always',\n",
       " 'also',\n",
       " 'well',\n",
       " 'smoke',\n",
       " 'set',\n",
       " 'does',\n",
       " 'times',\n",
       " 'survive',\n",
       " 'national',\n",
       " 'looks',\n",
       " 'free',\n",
       " 'destroyed',\n",
       " 'boy',\n",
       " 'terrorist',\n",
       " 'story',\n",
       " 'photo',\n",
       " 'murder',\n",
       " 'malaysia',\n",
       " 'injuries',\n",
       " 'face',\n",
       " 'check',\n",
       " 'bloody',\n",
       " 'bad',\n",
       " 'which',\n",
       " 'trapped',\n",
       " 'spill',\n",
       " 'someone',\n",
       " 'service',\n",
       " 'says',\n",
       " 'run',\n",
       " 'refugees',\n",
       " 'liked',\n",
       " 'fucking',\n",
       " 'flooding',\n",
       " 'failure',\n",
       " '70',\n",
       " 'women',\n",
       " 'until',\n",
       " 'tonight',\n",
       " 'show',\n",
       " 'oh',\n",
       " 'loud',\n",
       " 'fall',\n",
       " 'china',\n",
       " 'area',\n",
       " '30',\n",
       " '08',\n",
       " 'ûª',\n",
       " 'thunder',\n",
       " 'saudi',\n",
       " 'rain',\n",
       " 'o',\n",
       " 'movie',\n",
       " 'landslide',\n",
       " 'game',\n",
       " 'boat',\n",
       " 'around',\n",
       " 'week',\n",
       " 'weapon',\n",
       " 'terrorism',\n",
       " 'rescue',\n",
       " 'p',\n",
       " 'girl',\n",
       " 'confirmed',\n",
       " 'call',\n",
       " 'blood',\n",
       " 'bag',\n",
       " 'air',\n",
       " 'wind',\n",
       " 'survived',\n",
       " 'put',\n",
       " 'migrants',\n",
       " 'injury',\n",
       " 'head',\n",
       " 'hail',\n",
       " 'drought',\n",
       " 'weapons',\n",
       " 'survivors',\n",
       " 'screaming',\n",
       " 'report',\n",
       " 'released',\n",
       " 'panic',\n",
       " 'kills',\n",
       " 'hurricane',\n",
       " 'hostage',\n",
       " 'explosion',\n",
       " 'destroy',\n",
       " 'burned',\n",
       " 'big',\n",
       " 'whole',\n",
       " 'warning',\n",
       " 'save',\n",
       " 'rescued',\n",
       " 'missing',\n",
       " 'hazard',\n",
       " 'breaking',\n",
       " 'attacked',\n",
       " 'thing',\n",
       " 'past',\n",
       " 'mosque',\n",
       " 'made',\n",
       " 'heard',\n",
       " 'fuck',\n",
       " 'destruction',\n",
       " 'bridge',\n",
       " 'apocalypse',\n",
       " 'against',\n",
       " '40',\n",
       " 'violent',\n",
       " 'trauma',\n",
       " 'sinking',\n",
       " 'sinkhole',\n",
       " 'responders',\n",
       " 'rescuers',\n",
       " 'ok',\n",
       " 'hundreds',\n",
       " 'high',\n",
       " 'end',\n",
       " 'drowning',\n",
       " 'bags',\n",
       " 'wave',\n",
       " 'self',\n",
       " 'saw',\n",
       " 'real',\n",
       " 'massacre',\n",
       " 'keep',\n",
       " 'island',\n",
       " 'dust',\n",
       " 'demolished',\n",
       " 'deaths',\n",
       " 'crush',\n",
       " 'crashed',\n",
       " 'collapsed',\n",
       " 'catastrophic',\n",
       " '0',\n",
       " 'white',\n",
       " 'use',\n",
       " 'stock',\n",
       " 'river',\n",
       " 'lives',\n",
       " 'lava',\n",
       " 'hazardous',\n",
       " 'explode',\n",
       " 'evacuate',\n",
       " 'due',\n",
       " 'county',\n",
       " 'blown',\n",
       " 'wreckage',\n",
       " 'very',\n",
       " 'update',\n",
       " 'trouble',\n",
       " 'traumatised',\n",
       " 'tragedy',\n",
       " 'structural',\n",
       " 'screamed',\n",
       " 'quarantine',\n",
       " 'outbreak',\n",
       " 'must',\n",
       " 'market',\n",
       " 'long',\n",
       " 'light',\n",
       " 'exploded',\n",
       " 'drown',\n",
       " 'displaced',\n",
       " 'derailment',\n",
       " 'collision',\n",
       " 'collided',\n",
       " 'charged',\n",
       " 'catastrophe',\n",
       " 'bang',\n",
       " 'away',\n",
       " 'august',\n",
       " 'ambulance',\n",
       " 'screams',\n",
       " 'ruin',\n",
       " 'quarantined',\n",
       " 'meltdown',\n",
       " 'least',\n",
       " 'group',\n",
       " 'electrocuted',\n",
       " 'crushed',\n",
       " 'calgary',\n",
       " 'bombed',\n",
       " 'blew',\n",
       " 'bleeding',\n",
       " 'battle',\n",
       " '05',\n",
       " '00',\n",
       " 'wounds',\n",
       " 'three',\n",
       " 'suspect',\n",
       " 'riot',\n",
       " 'part',\n",
       " 'panicking',\n",
       " 'obliteration',\n",
       " 'mudslide',\n",
       " 'investigators',\n",
       " 'inundated',\n",
       " 'harm',\n",
       " 'flattened',\n",
       " 'engulfed',\n",
       " 'deluged',\n",
       " 'curfew',\n",
       " 'caused',\n",
       " 'blast',\n",
       " 'better',\n",
       " 'bagging',\n",
       " 'b',\n",
       " 'arson',\n",
       " 'anniversary',\n",
       " 'airplane',\n",
       " 'wrecked',\n",
       " 'windstorm',\n",
       " 'twister',\n",
       " 'thought',\n",
       " 'something',\n",
       " 'sirens',\n",
       " 'seismic',\n",
       " 'sandstorm',\n",
       " 'rioting',\n",
       " 'obliterated']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_map = make_hash_map(df) #create hash map (words and frequency) from tokenized dataset\n",
    "\n",
    "vocab=frequent_vocab(hash_map, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment! Change max_feature to 100. Check out what do you get. \n",
    "Remember to change it back to 500 or more afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we build our bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function bagofwords with the following input: sentence and words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) #tokenize sentences/ tweets and assign it to variable sentence_words\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words)) #create a NumPy array made up of zeroes with size len(words)\n",
    "    # loop through data and add value of 1 when token is present in the tweet\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # return the bag of word for one tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test your function using a made up text data.\n",
    "Look at the list of words above to see what words you might want to add into your sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 't co http  in for'\n",
    "bagofwords(text, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice your one row of bag of words! \n",
    "\n",
    "Now, we want to loop this function through our whole dataset. See below for how we do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a NumPy array with the specified dimension to contain the bag of words\n",
    "n_words = len(vocab)\n",
    "n_docs = len(df)\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "# use loop function to add new row for each tweet. \n",
    "for ii in range(n_docs): \n",
    "    #call out the previous function 'bagofwords'. see the inputs: sentence and words\n",
    "    bag_o[ii,:] = bagofwords(df['text'].iloc[ii], vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find out the [dimension](https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python) of the numpy array. Does it make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860, 500)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find out the total frequency, inverse document frequency\n",
    "\n",
    "Here, we would like to work with words that provide us with the most meaning in the sentences/ tweets. Does it make sense to think that the words that are used most often are important?\n",
    "\n",
    "We first take a look at the words inside our bag of words. Print the 20 most frequent words.  \n",
    "hint: Your hash_map is a dictionary of all words with each word as a key, and its frequency as a value aka dict{word: frequency}. What do you notice about these most common words?\n",
    "\n",
    "See [this article](https://docs.python.org/3/howto/sorting.html) under 'Key Functions' to find out. Use object’s indices as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 7447),\n",
       " ('co', 6800),\n",
       " ('http', 6154),\n",
       " ('in', 2807),\n",
       " ('i', 2507),\n",
       " ('s', 1276),\n",
       " ('for', 1243),\n",
       " ('on', 1236),\n",
       " ('that', 852),\n",
       " ('with', 797),\n",
       " ('by', 777),\n",
       " ('at', 748),\n",
       " ('this', 702),\n",
       " ('https', 618),\n",
       " ('from', 613),\n",
       " ('be', 596),\n",
       " ('was', 553),\n",
       " ('û_', 514),\n",
       " ('have', 513),\n",
       " ('amp', 510)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is key=lambda for? See [this article](https://stackoverflow.com/questions/13669252/what-is-key-lambda/13669294) to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the top 20 words above. What do you notice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing features\n",
    "The 20 most common words give almost NO information about the tweets. They are remains of twitter urls, as well as some common words frequently found in all text. We can hardly consider them 'important features'. It seems then that to improve the model, we should do more than just look at the most frequent words. \n",
    "\n",
    "Perhaps we should look for words that appear frequently in some documents, but not in all documents. Why do you think this makes sense?\n",
    "\n",
    "This is the intuition behind what is known as 'total frequency-inverse document frequency', or tfidf.  \n",
    "\n",
    "\n",
    "The tfidf formula is below: \n",
    "$$w_{i,j}=tf_{i,j}*log(\\frac{N}{df_i})$$  \n",
    "In this formula, $i$ is a word indexer and $j$ is a document indexer.  \n",
    "In your bag of words, each row is a document, while each column is the frequency of a word in that document. This is already the 'term frequency' portion of tfidf ($tf_{i,j}$). \n",
    "\n",
    "### Inverse document frequency\n",
    "\n",
    "We now want to calculate the inverse document frequency, which can be understood in the following way: for each word, count the number of documents it appears in, and then take the log of the inverse of that number.  \n",
    "\n",
    "Build the idf vector in 2 parts. \n",
    "1. First, build the word frequency for each word. \n",
    "2. Then divide the number of documents (N) by the word frequency and take the log of the result.\n",
    "\n",
    "Remember what we have done during the acquire phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize 2 variables representing the number of tweets (numdocs) and the number of tokens/words (numwords)\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "\n",
    "#Changing into the tfidf formula as above\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "\n",
    "#Count the number of documents the word appears in.\n",
    "for word in range(#your code here):\n",
    "    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n",
    "\n",
    "idf = np.log(N/word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will complete our tfidf by nultiplying our bag of words (term frequency) with the idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializs tfidf array\n",
    "tfidf = np.empty([#your code here])\n",
    "\n",
    "#loop through the tweets, multiply term frequency (represented by bag of words) with idf\n",
    "for doc in range(#your code here):\n",
    "    tfidf[doc, :]=bag_o[doc, :]*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860, 500)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe the tfidf array? It is made of the tfidf value of each of the 500 token in the 10860 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train your model with Machine Learning\n",
    "Now that you finally have your tfidf array. It is time to train your model and to make predictions! We will be using the scikit learn library, which provides a number of machine learning models. \n",
    "\n",
    "Do you remember what is machine learning? It is an application of AI that allows a system to automatically learn without being explicitly programmed. \n",
    "\n",
    "Now, we are using what we call the supervised learning. Do you remember what this is?\n",
    "This is the type of learning that enable us to make models to predict certain system, given a given training set. In this case, we want to predict if a text relates to news about disaster or not. We have already downloaded text data from twitter, and we have labelled the data with several labels i.e. 'Relevant', 'Not Relevant', and 'Can't decide'. These data will be used to train our model. Let's find out how we can do this!\n",
    "\n",
    "Let's first download the libraries required to do this. The scikit learn library contains numerous useful functions to be used for machine learning problem. \n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #to import logistic regression model\n",
    "from sklearn.model_selection import train_test_split #to split data into training and testing set\n",
    "from sklearn.model_selection import GridSearchCV #to find out the best parameter for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Split data into training and test set\n",
    "\n",
    "Before training our model, we will split our dataset into 2: a training set, and a test set.\n",
    "\n",
    "We will then train our model on the training set, and then test the model generated during the training stage on the test set. This is to ensure that the testing is done on dataset that the model has never 'seen'/ processed. \n",
    "\n",
    "A good starting point for the split is to have 80% of your data in the train set, and 20% of the data in the test set.  \n",
    "\n",
    "Let's do this now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_all and y_all into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(tfidf,df['relevance'].values,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you explain what is happening on the code above?. We are using the train_test_split function to split the tfidf array and part of the initial dataframe which include the 'relevance' value of our tweet. What does [shuffle=True](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) means?\n",
    "\n",
    "Let's learn more about the dataset we are working with! print tfidf and df['relevance'] below to see them. Meanwhile, find out what is [.value](https://www.geeksforgeeks.org/python-pandas-dataframe-values/) for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have already split our dataset, let's see what data we have now. Print the shape of X_train, X_test, y_train and y_test! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8145, 500)\n",
      "(2715, 500)\n",
      "(8145,)\n",
      "(2715,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare X_train and X_test, and also compare y_train and y_test. What do you notice about the difference in shape? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create a model instance\n",
    "After splitting the data, we will make an instance of the model i.e. we are simply initialising the model. In this task, we are using the logistic regressor, which is useful when we want to categorise data. Read [here](https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102) to find out more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance\n",
    "logreg = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train the model on data, store the information learnt from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model on the training set\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not worry about understanding the parameters shown above for now. You can still create projects with the logistic regressor even if you do not know every parameter involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use model to predict relevance based on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ca384041d34b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred=logreg.predict(X_test)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the y_pred mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now measure our model performance by finding out the accuracy value of the model. Remember, accuracy is defined as:\n",
    "\n",
    "Fraction of correct predictions = correct predictions / total number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! we have collected our data, process them, split them into training and testing data, train our model, and evaluate performance of our model. Next, we will define a function that will help us do all these task in one go! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Pipeline\n",
    "\n",
    "Your task is to build a function that will:\n",
    "1. Take in the untrained model, tfidf array, and the values from the training target.\n",
    "2. Randomly split the two into a train and test set\n",
    "3. Fit the model on the training set\n",
    "4. Print the accuracy score on the test set\n",
    "5. Return the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(rf, X_all, y_all): #Take in the untrained model, tfidf array, and the values from the training target.\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True) #Randomly split the two into a train and test set\n",
    "    logreg.fit(X_train,y_train) #Fit the model on the training set\n",
    "    print(rf.score(X_test,y_test)) #Print the score on the test set\n",
    "    return logreg #Return the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the function into our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7742173112338858\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver = 'lbfgs')\n",
    "#logreg = LogisticRegression()\n",
    "\n",
    "X_all = tfidf\n",
    "y_all = df['relevance'].values\n",
    "logreg = classify(logreg, X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters (Optional)\n",
    "If you have implemented this right, you should have a test score of at least 0.75. \n",
    "\n",
    "Let us try to improve this score by tuning the hyperparameters. We first take a look at the parameters in your logistic regressor. Feel free to read more about its parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Again, don't stress yourself out if you do not understand these parameters now. It will come with more reading and practise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be automatically tuned using scikit-learn's GridSearchCV. This function takes in a model, as well as a dictionary of parameters with values to test, and runs tests each combination of parameters to find the optimal combination for the highest score. Learn more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters here\n",
    "parameters = {'C':[0.001, 0.01, 0.1, 1, 10], 'tol':[0.0001, 0.001, 0.01], 'max_iter':[100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(logreg, parameters, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10], 'tol': [0.0001, 0.001, 0.01], 'max_iter': [100, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001, 'max_iter': 100, 'tol': 0.01} 0.7294659300184162\n"
     ]
    }
   ],
   "source": [
    "# You can view the raw results using clf.cv_results_\n",
    "print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the accuracy result. Do you notice that it drops? Why do think this is so?\n",
    "\n",
    "The accuracy might be different because we split our models randomly. What we can do is to repeat the model fitting multiple times and get the average accuracy result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build your pipeline\n",
    "At this point you have a trained and optimized model ready to predict tweets. You will now tie it all together to build a pipeline for prediction. \n",
    "\n",
    "This will be a function that:\n",
    "1. Take in a tweet in the form of a string\n",
    "2. Prints a prediction on whether the tweet is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_predictor(tweet):\n",
    "    word_vector = # your code here # set a variable with bag of words. Remember the bagofwords function you have created?\n",
    "    word_tfidf = # your code here #find tfidf value\n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) #predict wether a tweet is relevant or not relevant to natural disaster\n",
    "    results = {1:'Relevant', 0:'Not Relevant'} #creating a set containing the potential results. You can change the 'Relevant' and 'Not relevant' tag\n",
    "    print(results[int(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant\n",
      "Not Relevant\n"
     ]
    }
   ],
   "source": [
    "tweet1 = 'When the aftershock happened (Nepal) we were the last intl team still there; in a way we were 1st responders'\n",
    "tweet2 = 'NLP is fun and I learnt so much today.'\n",
    "twitter_predictor(tweet1)\n",
    "twitter_predictor(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Write your own tweets and see if your model can classify them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitter_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7accf227bebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtweet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Michael curry is on the roll as he scored the fifth goal on the football tournament.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#tweet2 = 'Michael curry is on fire as he scored the fifth goal on the football tournament.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtwitter_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtwitter_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twitter_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your model able to give you the right result? \n",
    "\n",
    "What do you think we can do to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now built your very own machine learning NLP model.  \n",
    "  \n",
    "# 5. NLP classification challenge!\n",
    "Now that you have learnt the basics of using the bag-of-words normalized by TFIDF to do classification of natural language data, it is time put your skills to the test!  \n",
    "\n",
    "## Sentiment analysis\n",
    "An important application of the classification of natural text is in _sentiment analysis_. Sentiment analysis is the process of categorizing opinions in a piece of text in order to identify the writer's attitude toward a particular topic.  \n",
    "  \n",
    "In this challenge, we will be classifying movie reviews from [imdb](https://www.imdb.com/). The data has already been stored as two .pkl files (for now, just understand these as file types that can be read using python), one for the training data, and one on the test data. \n",
    "\n",
    "You will have to process and train your model on the train dataset of movie reviews `df_raw.pkl`, and then report the accuracy of your model on the test move reviews `df_raw_test.pkl`.  \n",
    "  \n",
    "You will be predicting if a review has either a positive or negative sentiment. Positive sentiments are labeled as 1, while negative sentiments are labeled as 0.\n",
    "  \n",
    "In this segment we utilize a few new functions provided by sklearn. \n",
    "1. You can generate your bag of words and condition it using TFIDF with the functions you have built earlier. \n",
    "2. Alternatively, the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function can be used to create your bag of words. Use the `max_features=5000` argument to only select the top 5000 most common words. \n",
    "3. You can also use the [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) to help transform your bag of words with TFIDF.  \n",
    "  \n",
    "The dataframes for the train and test dataset has already been imported for you. You will have to make use of the skills you have learnt earlier and in Experience 1 to preprocess, vectorize (with the bag of words), transform (with TFIDF), fit, and predict the movie review sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer # This function helps you create your bag of words\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # This function automatically normalizes your bag of words\n",
    "df_raw = pd.read_pickle('./imdb/df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('./imdb/df_raw_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, try printing a sample of the test dataset. What are the names of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>scores</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I saw this film on September 1st, 2005 in Indi...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maybe I'm reading into this too much, but I wo...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I felt this film did have many good qualities....</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This movie is amazing because the fact that th...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Quitting\" may be as much about exiting a pre-...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I loved this movie from beginning to end.I am ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I was fortunate to attend the London premier o...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I first saw this movie on IFC. Which is a grea...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I must say, every time I see this movie, I am ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My wife is a mental health therapist and we wa...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I saw this film at the Rotterdam International...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Night of the Hunted\" stars French porn star B...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Even if you're a fan of Jean Rollin's idiosync...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I was surprised how much I enjoyed this. Sure ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I went into \"Night of the Hunted\" not knowing ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I have certainly not seen all of Jean Rollin's...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Since this cartoon was made in the old days, F...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Despite the title and unlike some other storie...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Felix in Hollywood is a great film. The versio...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A gem of a cartoon from the silent era---it wa...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>This short is one of the best of all time and ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Felix is watching an actor rehearse his lines:...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>While I can't say whether or not Larry Hama ev...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Errol Flynn's roguish charm really shines thro...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Warner Brothers tampered considerably with Ame...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>I was expecting \"Born to Kill\" to be an exciti...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>Fellow Giallo-fanatics: beware and/or proceed ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>I'm almost embarrassed to admit to seeing CALI...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12473</th>\n",
       "      <td>I rated this movie a 3 and that was generous. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12474</th>\n",
       "      <td>\"Caligula\" shares many of the same attributes ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12475</th>\n",
       "      <td>Writer &amp; director Robert Downey, Sr., a pionee...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12476</th>\n",
       "      <td>Probably not the same version as most of the o...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12477</th>\n",
       "      <td>Who really wants to see that? Disgusting viole...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12478</th>\n",
       "      <td>As a semi-film buff, I had heard of this infam...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>210 minute version (extremely hardcore, or so ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>Although its plot is taken from the history of...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>I found the movie at my local video store and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>As an ancient movie fan, I had heard much abou...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>Incomprehensibly dreadful mishmash of the prob...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>I'm currently slogging through Gibbon's 'Fall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12485</th>\n",
       "      <td>It's a rare sensation to come across a film so...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12486</th>\n",
       "      <td>Starting with a tearjerking poem and images of...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12487</th>\n",
       "      <td>I saw this film at the tender age of 18 with a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12488</th>\n",
       "      <td>It's rare to see film that strikes out in ever...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12489</th>\n",
       "      <td>Some describe CALIGULIA as \"the\" most controve...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12490</th>\n",
       "      <td>As I read the script on-line, I thought \"Capot...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12491</th>\n",
       "      <td>In this film, we're invited to observe the des...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12492</th>\n",
       "      <td>many people said this was a great movie with H...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12493</th>\n",
       "      <td>This is one dreary, inert, self-important bore...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12494</th>\n",
       "      <td>Awful, awful, awful times a hundred still does...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12499</th>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  scores  positive\n",
       "0      I went and saw this movie last night after bei...      10         1\n",
       "1      Actor turned director Bill Paxton follows up h...       7         1\n",
       "2      As a recreational golfer with some knowledge o...       9         1\n",
       "3      I saw this film in a sneak preview, and it is ...       8         1\n",
       "4      Bill Paxton has taken the true story of the 19...       8         1\n",
       "5      I saw this film on September 1st, 2005 in Indi...       9         1\n",
       "6      Maybe I'm reading into this too much, but I wo...       8         1\n",
       "7      I felt this film did have many good qualities....       7         1\n",
       "8      This movie is amazing because the fact that th...      10         1\n",
       "9      \"Quitting\" may be as much about exiting a pre-...       8         1\n",
       "10     I loved this movie from beginning to end.I am ...      10         1\n",
       "11     I was fortunate to attend the London premier o...       9         1\n",
       "12     I first saw this movie on IFC. Which is a grea...       9         1\n",
       "13     I must say, every time I see this movie, I am ...       9         1\n",
       "14     My wife is a mental health therapist and we wa...       9         1\n",
       "15     I saw this film at the Rotterdam International...       9         1\n",
       "16     \"Night of the Hunted\" stars French porn star B...       7         1\n",
       "17     Even if you're a fan of Jean Rollin's idiosync...       8         1\n",
       "18     I was surprised how much I enjoyed this. Sure ...       8         1\n",
       "19     I went into \"Night of the Hunted\" not knowing ...       8         1\n",
       "20     I have certainly not seen all of Jean Rollin's...       8         1\n",
       "21     Since this cartoon was made in the old days, F...       8         1\n",
       "22     Despite the title and unlike some other storie...      10         1\n",
       "23     Felix in Hollywood is a great film. The versio...       8         1\n",
       "24     A gem of a cartoon from the silent era---it wa...       9         1\n",
       "25     This short is one of the best of all time and ...      10         1\n",
       "26     Felix is watching an actor rehearse his lines:...       8         1\n",
       "27     While I can't say whether or not Larry Hama ev...       9         1\n",
       "28     Errol Flynn's roguish charm really shines thro...       8         1\n",
       "29     Warner Brothers tampered considerably with Ame...      10         1\n",
       "...                                                  ...     ...       ...\n",
       "12470  I was expecting \"Born to Kill\" to be an exciti...       4         0\n",
       "12471  Fellow Giallo-fanatics: beware and/or proceed ...       4         0\n",
       "12472  I'm almost embarrassed to admit to seeing CALI...       1         0\n",
       "12473  I rated this movie a 3 and that was generous. ...       3         0\n",
       "12474  \"Caligula\" shares many of the same attributes ...       4         0\n",
       "12475  Writer & director Robert Downey, Sr., a pionee...       3         0\n",
       "12476  Probably not the same version as most of the o...       2         0\n",
       "12477  Who really wants to see that? Disgusting viole...       1         0\n",
       "12478  As a semi-film buff, I had heard of this infam...       1         0\n",
       "12479  210 minute version (extremely hardcore, or so ...       3         0\n",
       "12480  Although its plot is taken from the history of...       4         0\n",
       "12481  I found the movie at my local video store and ...       1         0\n",
       "12482  As an ancient movie fan, I had heard much abou...       2         0\n",
       "12483  Incomprehensibly dreadful mishmash of the prob...       1         0\n",
       "12484  I'm currently slogging through Gibbon's 'Fall ...       1         0\n",
       "12485  It's a rare sensation to come across a film so...       1         0\n",
       "12486  Starting with a tearjerking poem and images of...       2         0\n",
       "12487  I saw this film at the tender age of 18 with a...       1         0\n",
       "12488  It's rare to see film that strikes out in ever...       1         0\n",
       "12489  Some describe CALIGULIA as \"the\" most controve...       4         0\n",
       "12490  As I read the script on-line, I thought \"Capot...       2         0\n",
       "12491  In this film, we're invited to observe the des...       3         0\n",
       "12492  many people said this was a great movie with H...       2         0\n",
       "12493  This is one dreary, inert, self-important bore...       2         0\n",
       "12494  Awful, awful, awful times a hundred still does...       2         0\n",
       "12495  I occasionally let my kids watch this garbage ...       1         0\n",
       "12496  When all we have anymore is pretty much realit...       1         0\n",
       "12497  The basic genre is a thriller intercut with an...       3         0\n",
       "12498  Four things intrigued me as to this film - fir...       3         0\n",
       "12499  David Bryce's comments nearby are exceptionall...       4         0\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, process your text using the `CountVectorizer` to create your bag of words. you can create a class of `CountVectorizer()`, and use the method `.fit_transform()` with the text as argument to build your bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "test_data_features = vectorizer.transform(df_raw_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize your bag of words using the `TfidfTransformer`. Use it the same way as above. Create a class, and use the `.fit_transform()` method with the bag of words as your argument to create your TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your transformed bag of words as the features to train and test your model like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = tfidf.toarray()\n",
    "y_all = df_raw['positive'].values\n",
    "X_test = tfidf_test.toarray()\n",
    "y_test = df_raw_test['positive'].values\n",
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(X_all,y_all)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    return rf\n",
    "classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have an accuracy of 80% without any hyperparameter tuning. Try and get the best accuracy on the test set that you can! This dataset is one of the hallmarks of natural language processing and is the entry-point for many aspiring data scientists and engineers. You can [see the original competition here](https://www.kaggle.com/c/word2vec-nlp-tutorial), and look at the different solutions other people have created!  \n",
    "\n",
    "## Next up, you can create a function to input your own review and get the model to predict if your sentence is positive or negative!\n",
    "\n",
    "# Congratulations!\n",
    "You have learnt how to build a natural language text classifier! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
